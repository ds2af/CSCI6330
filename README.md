# CSCI6330
Parallel Processing
### Introduction
Training neural networks on large datasets often requires a lot of time and computational resources. This is especially true for complex RNN models, which are widely used for time series prediction. In this paper, our aim is to reduce the computational overhead in training these networks by applying parallel computing techniques using the mpi4py library. By distributing the training process across multiple processors, our goal is to speed up model training while maintaining accuracy. We will evaluate our approach on wind farm data obtained from seven wind farms in Europe and stock prices from the S\&P 500 index using Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE), Mean Square Error (MSE), and Root Mean Square Error (RMSE) metrics to observe how the parallel computing approach can reduce training time without compromising performance.



To run the code follow the following steps:
#### Step 1:
Login to Google Colab.
#### Step 2:
Upload the provided notebook.
#### Step 3:
Upload the dataset in the environment.
#### Step 4:
Run each cell one by one.
